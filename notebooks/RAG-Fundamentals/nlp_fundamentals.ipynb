{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33eea61-332f-42b7-855e-46072aa671bb",
   "metadata": {},
   "source": [
    "✅ Phase 0: NLP Foundations – \"Speaking the Language of AI\"\n",
    "This phase is designed to build your intuition, technical skill, and vocabulary around Natural Language Processing (NLP), the core layer of Retrieval-Augmented Generation (RAG). Here’s what we’ll cover:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f103c19-61c6-420c-b3b9-c8d576706b47",
   "metadata": {},
   "source": [
    "🔹 What is NLP?\n",
    "Natural Language Processing (NLP) is the intersection of linguistics and machine learning that enables machines to understand, interpret, and generate human language.\n",
    "\n",
    "🔹 Why is NLP the Foundation of RAG?\n",
    "RAG uses language models for generation (G in RAG).\n",
    "\n",
    "It uses retrieval mechanisms that depend on understanding semantics and meaning (NLP!).\n",
    "\n",
    "Concepts like tokenization, embeddings, context windows, etc., come directly from NLP.\n",
    "\n",
    "🔹 What You’ll Learn in This Phase:\n",
    "| Concept                            | Why It Matters for RAG                                | Activities                                             |\n",
    "| ---------------------------------- | ----------------------------------------------------- | ------------------------------------------------------ |\n",
    "| **Tokenization**                   | Breaks text into pieces that the model can process    | Try tokenizing using `nltk` and `transformers`         |\n",
    "| **Embeddings**                     | Convert text into numerical vectors for comparison    | Experiment with OpenAI, Hugging Face, and local models |\n",
    "| **Context Windows**                | Determines how much text a model can \"see\" at once    | Visualize limits of different LLMs                     |\n",
    "| **Text Cleaning**                  | Prepares data for indexing and retrieval              | Build a simple pipeline using `re` and `nltk`          |\n",
    "| **Similarity Search**              | Core to retrieval in RAG                              | Use cosine similarity between embeddings               |\n",
    "| **Named Entity Recognition (NER)** | Helps extract meaningful chunks from documents        | Use `spaCy` or `transformers` for practical use        |\n",
    "| **Vectorization Techniques**       | From TF-IDF → Word2Vec → Sentence Transformers        | Compare their outputs and quality                      |\n",
    "| **Prompt Engineering Basics**      | Formulating questions that produce accurate responses | Design and refine prompts with LLMs                    |\n",
    "\n",
    "\n",
    "🔸 Suggested Tools for Hands-on Practice:\n",
    "🐍 Python Libraries:\n",
    "\n",
    "nltk, spaCy, transformers, sentence-transformers, sklearn\n",
    "\n",
    "🧪 Try This Notebook:\n",
    "\n",
    "notebooks/RAG/nlp_fundamentals.ipynb → where you’ll practice hands-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9482266-8c8a-433e-a9f6-307201ccf5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/koyas/genai-lab-sandbox/venv/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/koyas/genai-lab-sandbox/venv/lib/python3.12/site-packages (from scikit-learn) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/koyas/genai-lab-sandbox/venv/lib/python3.12/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/koyas/genai-lab-sandbox/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/koyas/genai-lab-sandbox/venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc3323-f799-4469-9996-f1b0e0742aea",
   "metadata": {},
   "source": [
    "✅ 1. Tokenization :\n",
    "\n",
    "Breaks text into pieces that the model can process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f1626b9-53db-46d2-a9a6-9a14bd259fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3c83dea6814e5bb09f406d7f5ac0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8467ca3578744fc686da843f85f926c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae73968c43a45e7a529728001641ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0eba1f2f4264eed8ba38014e826e3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', ',', 'this', 'is', 'a', 'testing', 'for', 'rag', 'learning', 'with', 'a', 'sample', 'sentence']\n",
      "TokenIds: [7592, 1010, 2023, 2003, 1037, 5604, 2005, 17768, 4083, 2007, 1037, 7099, 6251]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Hello, this is a testing for RAG learning with a sample sentence\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokenids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\",tokens)\n",
    "print(\"TokenIds:\",tokenids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7feb53-8378-46fd-a772-f9a49053320d",
   "metadata": {},
   "source": [
    "✅ 2. Normalization\n",
    "\n",
    "Text normalization is the process of preparing text for analysis by converting it into a standard format.\n",
    "\n",
    "🔹 Why do we normalize?\n",
    "\n",
    "To remove inconsistencies (e.g., \"Hello\", \"hello!\", \"HELLO\" → all mean the same)\n",
    "\n",
    "Reduces noise before downstream NLP tasks (e.g., vectorization)\n",
    "\n",
    "🔹 What’s involved?\n",
    "\n",
    "Lowercasing\n",
    "\n",
    "Removing punctuation\n",
    "\n",
    "Stemming\n",
    "\n",
    "Lemmatization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
